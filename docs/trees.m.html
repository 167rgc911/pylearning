<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>pylearning.trees API documentation</title>
    <meta name="description" content="" />

  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 1em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  } 

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; } 

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;
      
      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>


  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
</head>
<body>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <h1>Index</h1>
    <ul id="index">


    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#pylearning.trees.DecisionTree">DecisionTree</a></span>
        
          
  <ul>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.__init__">__init__</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.build_tree">build_tree</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.choose_random_features">choose_random_features</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.divide_set">divide_set</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.entropy">entropy</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.fit">fit</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.get_features_subset">get_features_subset</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.mean_output">mean_output</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.predict">predict</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.propagate">propagate</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.set_number_features_evaluated_split">set_number_features_evaluated_split</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.unique_counts">unique_counts</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTree.variance">variance</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#pylearning.trees.DecisionTreeClassifier">DecisionTreeClassifier</a></span>
        
          
  <ul>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.__init__">__init__</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.build_tree">build_tree</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.choose_random_features">choose_random_features</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.divide_set">divide_set</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.entropy">entropy</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.fit">fit</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.get_features_subset">get_features_subset</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.mean_output">mean_output</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.predict">predict</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.propagate">propagate</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.set_number_features_evaluated_split">set_number_features_evaluated_split</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.unique_counts">unique_counts</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeClassifier.variance">variance</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#pylearning.trees.DecisionTreeRegressor">DecisionTreeRegressor</a></span>
        
          
  <ul>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.__init__">__init__</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.build_tree">build_tree</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.choose_random_features">choose_random_features</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.divide_set">divide_set</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.entropy">entropy</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.fit">fit</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.get_features_subset">get_features_subset</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.mean_output">mean_output</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.predict">predict</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.propagate">propagate</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.set_number_features_evaluated_split">set_number_features_evaluated_split</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.unique_counts">unique_counts</a></li>
    <li class="mono"><a href="#pylearning.trees.DecisionTreeRegressor.variance">variance</a></li>
  </ul>

        </li>
      </ul>
    </li>

    </ul>
  </div>

    <article id="content">
      
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">pylearning.trees</span> module</h1>
  
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees" class="source">
    <pre><code>import random
import abc
from math import log2, sqrt

from .node import DecisionNode


class DecisionTree(metaclass=abc.ABCMeta):
    """
    Abstract base class for decision trees. This class is not meant to be
    instanciated,only its subclasses can be used.
    """


    def __init__(self, max_depth=-1, min_leaf_examples=6, max_split_features="auto"):
        self.root_node = None
        self.max_depth = max_depth
        self.min_leaf_examples = min_leaf_examples

        if max_split_features in ["auto","sqrt","log2"] or \
            isinstance(max_split_features, int) or max_split_features is None:
            self.max_split_features = max_split_features
            self.considered_features = None
        else:
            raise ValueError("Argument max_split_features must be 'auto', \
                            'sqrt', 'log2', an int or None")


    def fit(self, features, targets):
        """
        Trains the algorithm using the given dataset.
        :param features:    Features used to train the tree.
                            Array-like object of dimensions (nb_samples, nb_features)
        :param targets:     Target values corresponding to the features.
                            Array-like object of dimensions (n_samples)
        """
        if len(features) < 1:
            raise ValueError("Not enough samples in the given dataset")
        self.set_number_features_evaluated_split(features[0])
        self.root_node = self.build_tree(features, targets, self.max_depth)


    def predict(self, features):
        """
        Predict a value for the given features.
        :param features:    Array of features of dimension (nb_features)
        :return:            Float value or predicted class
        """
        return self.propagate(features, self.root_node)


    @abc.abstractmethod
    def build_tree(self, features, targets, depth):
        """ Abstract method used to build the tree """
        pass


    def set_number_features_evaluated_split(self, row):
        """
        Sets the number of considered features at each split depending on the
        max_split_features parameter.
        :param row: A single row of the features of shape (nb_features)
        """
        if isinstance(self.max_split_features, int):
            self.considered_features = self.max_split_features if \
                self.max_split_features <= len(row) else len(row)
        elif isinstance(self.max_split_features, str):
            if self.max_split_features in ['auto','sqrt']:
                self.considered_features = int(sqrt(len(row)))
            elif self.max_split_features == 'log2':
                self.considered_features = int(log2(len(row)))
        else:
            self.considered_features = len(row)


    def choose_random_features(self, row):
        """
        Randomly selects indexes in the given list. The number of indexes
        chosen is the square root of the number of elements in the initial
        list.
        :param row: One-dimensional array
        :return:    Array containing the chosen indexes
        """
        return random.sample(range(len(row)), self.considered_features)


    def get_features_subset(self, row):
        """
        Returns the randomly selected values in the given features.
        :param row: One-dimensional array of features
        """
        return [row[i] for i in self.features_indexes]


    def entropy(self, targets):
        """
        Returns the entropy in the given rows.
        :param targets:     1D array-like targets
        :return:            Float value of entropy
        """
        results = self.unique_counts(targets)
        ent = 0.0
        for val in results.values():
            p = float(val) / len(targets)
            ent -= p * log2(p)
        return ent


    def mean_output(self, targets):
        """
        Calculate the mean value of the given list.
        :param targets: One-dimensional array of floats or ints
        :return:        Float value
        """
        return sum(targets) / len(targets)


    def variance(self, targets):
        """
        Calculate the variance in the given list.
        :param targets: One-dimensional array of float or ints
        :return:        Float value
        """
        if len(targets) == 0:
            return None
        mean = self.mean_output(targets)
        variance = sum([(x - mean)**2 for x in targets])
        return variance


    def unique_counts(self, targets):
        """
        Returns the occurence of each result in the given dataset.
        :param targets:     1D array-like targets
        :return:            Dictionary of target => number of occurences
        """
        counts = {k: targets.count(k) for k in set(targets)}
        return counts


    def divide_set(self, features, targets, column, feature_value):
        """
        Divide the given dataset depending on the value at the given column index.
        :param features:    Features of the dataset
        :param targets:     Targets of the dataset
        :param column:      The index of the column used to split data
        :param value:       The value used for the split
        """
        split_function = None
        if isinstance(feature_value, int) or isinstance(feature_value, float):
            split_function = lambda row: row[column] >= feature_value
        else:
            split_function = lambda row: row[column] == feature_value

        set1 = [row for row in zip(features, targets) if split_function(row[0])]
        set2 = [row for row in zip(features, targets) if not split_function(row[0])]

        feat1, targs1 = [x[0] for x in set1], [x[1] for x in set1]
        feat2, targs2 = [x[0] for x in set2], [x[1] for x in set2]
        return feat1, targs1, feat2, targs2


    def propagate(self, observation, tree):
        """
        Makes a prediction using the given features.
        :param  observation:    The features to use to predict
        :param  tree:           The current node
        :return:                Predicted value (float)
        """
        if tree.result is not None:
            return tree.result
        else:
            v = observation[tree.col]
            branch = None
            if isinstance(v, int) or isinstance(v, float):
                if v >= tree.value:
                    branch = tree.tb
                else:
                    branch = tree.fb
            else:
                if v == tree.value:
                    branch = tree.tb
                else:
                    branch = tree.fb
            return self.propagate(observation, branch)



class DecisionTreeRegressor(DecisionTree):
    """
    :param  max_depth:          Maximum number of splits during training
    :param min_leaf_examples:   Minimum number of examples in a leaf node.
    :param max_split_features:  Maximum number of features considered at each
                                split (default='auto') :
                                   - If int, the given number of will be used
                                   - If 'auto' or 'sqrt', number of features
                                     considered = sqrt(nb_features)
                                   - If 'log2', considered = log2(nb_features)
                                   - If None, all features will be considered
    """


    def build_tree(self, features, targets, depth):
        """
        Recursively create the decision tree by splitting the dataset until there
        is no real reduce in variance, or there is less examples in a node than
        the minimum number of examples, or until the max depth is reached.
        :param features:    Array-like object of features of shape (nb_samples, nb_features)
        :param targets:     Array-like object of target values of shape (nb_samples)
        :param depth:       The current depth in the tree
        :return:            The root node of the constructed tree
        """
        if len(features) == 0:
            return DecisionNode()
        if depth == 0:
            return DecisionNode(result=self.mean_output(targets))

        lowest_variance = None
        best_criteria = None
        best_sets = None

        considered_features = self.choose_random_features(features[0])
        for column in considered_features:
            column_values = [feature[column] for feature in features]
            for feature_value in column_values:
                feats1, targs1, feats2, targs2 = \
                    self.divide_set(features, targets, column, feature_value)
                var1 = self.variance(targs1)
                var2 = self.variance(targs2)
                if var1 is None or var2 is None:
                    continue
                variance = var1 + var2
                if lowest_variance is None or variance < lowest_variance:
                    lowest_variance = variance
                    best_criteria   = (column, feature_value)
                    best_sets       = ((feats1, targs1),(feats2, targs2))

        # Check variance value also
        if lowest_variance is not None and \
            len(best_sets[0][0]) >= self.min_leaf_examples and \
            len(best_sets[1][0]) >= self.min_leaf_examples:
            left_branch = self.build_tree(best_sets[0][0], best_sets[0][1], depth - 1)
            right_branch = self.build_tree(best_sets[1][0], best_sets[1][1], depth - 1)
            return DecisionNode(col=best_criteria[0], value=best_criteria[1],
                                tb=left_branch, fb=right_branch)
        else:
            return DecisionNode(result=self.mean_output(targets))



class DecisionTreeClassifier(DecisionTree):
    """
    :param  max_depth:          Maximum number of splits during training
    :param min_leaf_examples:   Minimum number of examples in a leaf node.
    :param max_split_features:  Maximum number of features considered at each
                                split (default='auto') :
                                   - If int, the given number of will be used
                                   - If 'auto' or 'sqrt', number of features
                                     considered = sqrt(nb_features)
                                   - If 'log2', considered = log2(nb_features)
                                   - If None, all features will be considered
    """


    def build_tree(self, features, targets, depth):
        """
        Recursively create the decision tree by splitting the dataset until there
        is no real reduce in variance, or there is less examples in a node than
        the minimum number of examples, or until the max depth is reached.

        :param features:    Array-like object of features of shape (nb_samples, nb_features)
        :param targets:     Array-like object of target values of shape (nb_samples)
        :param depth:       The current depth in the tree
        :return:            The root node of the constructed tree
        """
        if len(features) == 0:
            return DecisionNode()
        if depth == 0:
            return DecisionNode(result=max(self.unique_counts(targets)))

        current_score = self.entropy(targets)
        best_gain = 0.0
        best_criteria = None
        best_sets = None

        considered_features = self.choose_random_features(features[0])
        for col in considered_features:
            column_values = set([row[col] for row in features])
            for value in column_values:
                feats1, targs1, feats2, targs2 = \
                    self.divide_set(features, targets, col, value)
                p = float(len(feats1)) / len(features)
                gain = current_score - p * self.entropy(targs1) - \
                        (1 - p) * self.entropy(targs2)
                if gain > best_gain and len(feats1) > 0 and len(feats2) > 0:
                    best_gain = gain
                    best_criteria = (col, value)
                    best_sets = ((feats1, targs1), (feats2, targs2))

        if best_gain > 0:
            left_branch = self.build_tree(best_sets[0][0], best_sets[0][1], depth - 1)
            right_branch = self.build_tree(best_sets[1][0], best_sets[1][1], depth - 1)
            return DecisionNode(col=best_criteria[0], value=best_criteria[1],
                                tb=left_branch, fb=right_branch)
        else:
            return DecisionNode(result=max(self.unique_counts(targets)))
</code></pre>
  </div>

  </header>

  <section id="section-items">


    <h2 class="section-title" id="header-classes">Classes</h2>
      
      <div class="item">
      <p id="pylearning.trees.DecisionTree" class="name">class <span class="ident">DecisionTree</span></p>
      
  
    <div class="desc"><p>Abstract base class for decision trees. This class is not meant to be
instanciated,only its subclasses can be used.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree" class="source">
    <pre><code>class DecisionTree(metaclass=abc.ABCMeta):
    """
    Abstract base class for decision trees. This class is not meant to be
    instanciated,only its subclasses can be used.
    """


    def __init__(self, max_depth=-1, min_leaf_examples=6, max_split_features="auto"):
        self.root_node = None
        self.max_depth = max_depth
        self.min_leaf_examples = min_leaf_examples

        if max_split_features in ["auto","sqrt","log2"] or \
            isinstance(max_split_features, int) or max_split_features is None:
            self.max_split_features = max_split_features
            self.considered_features = None
        else:
            raise ValueError("Argument max_split_features must be 'auto', \
                            'sqrt', 'log2', an int or None")


    def fit(self, features, targets):
        """
        Trains the algorithm using the given dataset.
        :param features:    Features used to train the tree.
                            Array-like object of dimensions (nb_samples, nb_features)
        :param targets:     Target values corresponding to the features.
                            Array-like object of dimensions (n_samples)
        """
        if len(features) < 1:
            raise ValueError("Not enough samples in the given dataset")
        self.set_number_features_evaluated_split(features[0])
        self.root_node = self.build_tree(features, targets, self.max_depth)


    def predict(self, features):
        """
        Predict a value for the given features.
        :param features:    Array of features of dimension (nb_features)
        :return:            Float value or predicted class
        """
        return self.propagate(features, self.root_node)


    @abc.abstractmethod
    def build_tree(self, features, targets, depth):
        """ Abstract method used to build the tree """
        pass


    def set_number_features_evaluated_split(self, row):
        """
        Sets the number of considered features at each split depending on the
        max_split_features parameter.
        :param row: A single row of the features of shape (nb_features)
        """
        if isinstance(self.max_split_features, int):
            self.considered_features = self.max_split_features if \
                self.max_split_features <= len(row) else len(row)
        elif isinstance(self.max_split_features, str):
            if self.max_split_features in ['auto','sqrt']:
                self.considered_features = int(sqrt(len(row)))
            elif self.max_split_features == 'log2':
                self.considered_features = int(log2(len(row)))
        else:
            self.considered_features = len(row)


    def choose_random_features(self, row):
        """
        Randomly selects indexes in the given list. The number of indexes
        chosen is the square root of the number of elements in the initial
        list.
        :param row: One-dimensional array
        :return:    Array containing the chosen indexes
        """
        return random.sample(range(len(row)), self.considered_features)


    def get_features_subset(self, row):
        """
        Returns the randomly selected values in the given features.
        :param row: One-dimensional array of features
        """
        return [row[i] for i in self.features_indexes]


    def entropy(self, targets):
        """
        Returns the entropy in the given rows.
        :param targets:     1D array-like targets
        :return:            Float value of entropy
        """
        results = self.unique_counts(targets)
        ent = 0.0
        for val in results.values():
            p = float(val) / len(targets)
            ent -= p * log2(p)
        return ent


    def mean_output(self, targets):
        """
        Calculate the mean value of the given list.
        :param targets: One-dimensional array of floats or ints
        :return:        Float value
        """
        return sum(targets) / len(targets)


    def variance(self, targets):
        """
        Calculate the variance in the given list.
        :param targets: One-dimensional array of float or ints
        :return:        Float value
        """
        if len(targets) == 0:
            return None
        mean = self.mean_output(targets)
        variance = sum([(x - mean)**2 for x in targets])
        return variance


    def unique_counts(self, targets):
        """
        Returns the occurence of each result in the given dataset.
        :param targets:     1D array-like targets
        :return:            Dictionary of target => number of occurences
        """
        counts = {k: targets.count(k) for k in set(targets)}
        return counts


    def divide_set(self, features, targets, column, feature_value):
        """
        Divide the given dataset depending on the value at the given column index.
        :param features:    Features of the dataset
        :param targets:     Targets of the dataset
        :param column:      The index of the column used to split data
        :param value:       The value used for the split
        """
        split_function = None
        if isinstance(feature_value, int) or isinstance(feature_value, float):
            split_function = lambda row: row[column] >= feature_value
        else:
            split_function = lambda row: row[column] == feature_value

        set1 = [row for row in zip(features, targets) if split_function(row[0])]
        set2 = [row for row in zip(features, targets) if not split_function(row[0])]

        feat1, targs1 = [x[0] for x in set1], [x[1] for x in set1]
        feat2, targs2 = [x[0] for x in set2], [x[1] for x in set2]
        return feat1, targs1, feat2, targs2


    def propagate(self, observation, tree):
        """
        Makes a prediction using the given features.
        :param  observation:    The features to use to predict
        :param  tree:           The current node
        :return:                Predicted value (float)
        """
        if tree.result is not None:
            return tree.result
        else:
            v = observation[tree.col]
            branch = None
            if isinstance(v, int) or isinstance(v, float):
                if v >= tree.value:
                    branch = tree.tb
                else:
                    branch = tree.fb
            else:
                if v == tree.value:
                    branch = tree.tb
                else:
                    branch = tree.fb
            return self.propagate(observation, branch)
</code></pre>
  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#pylearning.trees.DecisionTree">DecisionTree</a></li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, max_depth=-1, min_leaf_examples=6, max_split_features=&#39;auto&#39;)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.__init__', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.__init__" class="source">
    <pre><code>def __init__(self, max_depth=-1, min_leaf_examples=6, max_split_features="auto"):
    self.root_node = None
    self.max_depth = max_depth
    self.min_leaf_examples = min_leaf_examples
    if max_split_features in ["auto","sqrt","log2"] or \
        isinstance(max_split_features, int) or max_split_features is None:
        self.max_split_features = max_split_features
        self.considered_features = None
    else:
        raise ValueError("Argument max_split_features must be 'auto', \
                        'sqrt', 'log2', an int or None")
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.build_tree">
    <p>def <span class="ident">build_tree</span>(</p><p>self, features, targets, depth)</p>
    </div>
    

    
  
    <div class="desc"><p>Abstract method used to build the tree</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.build_tree', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.build_tree" class="source">
    <pre><code>@abc.abstractmethod
def build_tree(self, features, targets, depth):
    """ Abstract method used to build the tree """
    pass
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.choose_random_features">
    <p>def <span class="ident">choose_random_features</span>(</p><p>self, row)</p>
    </div>
    

    
  
    <div class="desc"><p>Randomly selects indexes in the given list. The number of indexes
chosen is the square root of the number of elements in the initial
list.
:param row: One-dimensional array
:return:    Array containing the chosen indexes</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.choose_random_features', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.choose_random_features" class="source">
    <pre><code>def choose_random_features(self, row):
    """
    Randomly selects indexes in the given list. The number of indexes
    chosen is the square root of the number of elements in the initial
    list.
    :param row: One-dimensional array
    :return:    Array containing the chosen indexes
    """
    return random.sample(range(len(row)), self.considered_features)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.divide_set">
    <p>def <span class="ident">divide_set</span>(</p><p>self, features, targets, column, feature_value)</p>
    </div>
    

    
  
    <div class="desc"><p>Divide the given dataset depending on the value at the given column index.
:param features:    Features of the dataset
:param targets:     Targets of the dataset
:param column:      The index of the column used to split data
:param value:       The value used for the split</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.divide_set', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.divide_set" class="source">
    <pre><code>def divide_set(self, features, targets, column, feature_value):
    """
    Divide the given dataset depending on the value at the given column index.
    :param features:    Features of the dataset
    :param targets:     Targets of the dataset
    :param column:      The index of the column used to split data
    :param value:       The value used for the split
    """
    split_function = None
    if isinstance(feature_value, int) or isinstance(feature_value, float):
        split_function = lambda row: row[column] >= feature_value
    else:
        split_function = lambda row: row[column] == feature_value
    set1 = [row for row in zip(features, targets) if split_function(row[0])]
    set2 = [row for row in zip(features, targets) if not split_function(row[0])]
    feat1, targs1 = [x[0] for x in set1], [x[1] for x in set1]
    feat2, targs2 = [x[0] for x in set2], [x[1] for x in set2]
    return feat1, targs1, feat2, targs2
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.entropy">
    <p>def <span class="ident">entropy</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the entropy in the given rows.
:param targets:     1D array-like targets
:return:            Float value of entropy</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.entropy', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.entropy" class="source">
    <pre><code>def entropy(self, targets):
    """
    Returns the entropy in the given rows.
    :param targets:     1D array-like targets
    :return:            Float value of entropy
    """
    results = self.unique_counts(targets)
    ent = 0.0
    for val in results.values():
        p = float(val) / len(targets)
        ent -= p * log2(p)
    return ent
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, features, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Trains the algorithm using the given dataset.
:param features:    Features used to train the tree.
                    Array-like object of dimensions (nb_samples, nb_features)
:param targets:     Target values corresponding to the features.
                    Array-like object of dimensions (n_samples)</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.fit', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.fit" class="source">
    <pre><code>def fit(self, features, targets):
    """
    Trains the algorithm using the given dataset.
    :param features:    Features used to train the tree.
                        Array-like object of dimensions (nb_samples, nb_features)
    :param targets:     Target values corresponding to the features.
                        Array-like object of dimensions (n_samples)
    """
    if len(features) < 1:
        raise ValueError("Not enough samples in the given dataset")
    self.set_number_features_evaluated_split(features[0])
    self.root_node = self.build_tree(features, targets, self.max_depth)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.get_features_subset">
    <p>def <span class="ident">get_features_subset</span>(</p><p>self, row)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the randomly selected values in the given features.
:param row: One-dimensional array of features</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.get_features_subset', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.get_features_subset" class="source">
    <pre><code>def get_features_subset(self, row):
    """
    Returns the randomly selected values in the given features.
    :param row: One-dimensional array of features
    """
    return [row[i] for i in self.features_indexes]
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.mean_output">
    <p>def <span class="ident">mean_output</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Calculate the mean value of the given list.
:param targets: One-dimensional array of floats or ints
:return:        Float value</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.mean_output', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.mean_output" class="source">
    <pre><code>def mean_output(self, targets):
    """
    Calculate the mean value of the given list.
    :param targets: One-dimensional array of floats or ints
    :return:        Float value
    """
    return sum(targets) / len(targets)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.predict">
    <p>def <span class="ident">predict</span>(</p><p>self, features)</p>
    </div>
    

    
  
    <div class="desc"><p>Predict a value for the given features.
:param features:    Array of features of dimension (nb_features)
:return:            Float value or predicted class</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.predict', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.predict" class="source">
    <pre><code>def predict(self, features):
    """
    Predict a value for the given features.
    :param features:    Array of features of dimension (nb_features)
    :return:            Float value or predicted class
    """
    return self.propagate(features, self.root_node)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.propagate">
    <p>def <span class="ident">propagate</span>(</p><p>self, observation, tree)</p>
    </div>
    

    
  
    <div class="desc"><p>Makes a prediction using the given features.
:param  observation:    The features to use to predict
:param  tree:           The current node
:return:                Predicted value (float)</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.propagate', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.propagate" class="source">
    <pre><code>def propagate(self, observation, tree):
    """
    Makes a prediction using the given features.
    :param  observation:    The features to use to predict
    :param  tree:           The current node
    :return:                Predicted value (float)
    """
    if tree.result is not None:
        return tree.result
    else:
        v = observation[tree.col]
        branch = None
        if isinstance(v, int) or isinstance(v, float):
            if v >= tree.value:
                branch = tree.tb
            else:
                branch = tree.fb
        else:
            if v == tree.value:
                branch = tree.tb
            else:
                branch = tree.fb
        return self.propagate(observation, branch)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.set_number_features_evaluated_split">
    <p>def <span class="ident">set_number_features_evaluated_split</span>(</p><p>self, row)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the number of considered features at each split depending on the
max_split_features parameter.
:param row: A single row of the features of shape (nb_features)</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.set_number_features_evaluated_split', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.set_number_features_evaluated_split" class="source">
    <pre><code>def set_number_features_evaluated_split(self, row):
    """
    Sets the number of considered features at each split depending on the
    max_split_features parameter.
    :param row: A single row of the features of shape (nb_features)
    """
    if isinstance(self.max_split_features, int):
        self.considered_features = self.max_split_features if \
            self.max_split_features <= len(row) else len(row)
    elif isinstance(self.max_split_features, str):
        if self.max_split_features in ['auto','sqrt']:
            self.considered_features = int(sqrt(len(row)))
        elif self.max_split_features == 'log2':
            self.considered_features = int(log2(len(row)))
    else:
        self.considered_features = len(row)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.unique_counts">
    <p>def <span class="ident">unique_counts</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the occurence of each result in the given dataset.
:param targets:     1D array-like targets
:return:            Dictionary of target =&gt; number of occurences</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.unique_counts', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.unique_counts" class="source">
    <pre><code>def unique_counts(self, targets):
    """
    Returns the occurence of each result in the given dataset.
    :param targets:     1D array-like targets
    :return:            Dictionary of target => number of occurences
    """
    counts = {k: targets.count(k) for k in set(targets)}
    return counts
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTree.variance">
    <p>def <span class="ident">variance</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Calculate the variance in the given list.
:param targets: One-dimensional array of float or ints
:return:        Float value</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTree.variance', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTree.variance" class="source">
    <pre><code>def variance(self, targets):
    """
    Calculate the variance in the given list.
    :param targets: One-dimensional array of float or ints
    :return:        Float value
    """
    if len(targets) == 0:
        return None
    mean = self.mean_output(targets)
    variance = sum([(x - mean)**2 for x in targets])
    return variance
</code></pre>
  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="pylearning.trees.DecisionTree.max_depth" class="name">var <span class="ident">max_depth</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="pylearning.trees.DecisionTree.min_leaf_examples" class="name">var <span class="ident">min_leaf_examples</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="pylearning.trees.DecisionTree.root_node" class="name">var <span class="ident">root_node</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
      
      <div class="item">
      <p id="pylearning.trees.DecisionTreeClassifier" class="name">class <span class="ident">DecisionTreeClassifier</span></p>
      
  
    <div class="desc"><p>:param  max_depth:          Maximum number of splits during training
:param min_leaf_examples:   Minimum number of examples in a leaf node.
:param max_split_features:  Maximum number of features considered at each
                            split (default='auto') :
                               - If int, the given number of will be used
                               - If 'auto' or 'sqrt', number of features
                                 considered = sqrt(nb_features)
                               - If 'log2', considered = log2(nb_features)
                               - If None, all features will be considered</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier" class="source">
    <pre><code>class DecisionTreeClassifier(DecisionTree):
    """
    :param  max_depth:          Maximum number of splits during training
    :param min_leaf_examples:   Minimum number of examples in a leaf node.
    :param max_split_features:  Maximum number of features considered at each
                                split (default='auto') :
                                   - If int, the given number of will be used
                                   - If 'auto' or 'sqrt', number of features
                                     considered = sqrt(nb_features)
                                   - If 'log2', considered = log2(nb_features)
                                   - If None, all features will be considered
    """


    def build_tree(self, features, targets, depth):
        """
        Recursively create the decision tree by splitting the dataset until there
        is no real reduce in variance, or there is less examples in a node than
        the minimum number of examples, or until the max depth is reached.

        :param features:    Array-like object of features of shape (nb_samples, nb_features)
        :param targets:     Array-like object of target values of shape (nb_samples)
        :param depth:       The current depth in the tree
        :return:            The root node of the constructed tree
        """
        if len(features) == 0:
            return DecisionNode()
        if depth == 0:
            return DecisionNode(result=max(self.unique_counts(targets)))

        current_score = self.entropy(targets)
        best_gain = 0.0
        best_criteria = None
        best_sets = None

        considered_features = self.choose_random_features(features[0])
        for col in considered_features:
            column_values = set([row[col] for row in features])
            for value in column_values:
                feats1, targs1, feats2, targs2 = \
                    self.divide_set(features, targets, col, value)
                p = float(len(feats1)) / len(features)
                gain = current_score - p * self.entropy(targs1) - \
                        (1 - p) * self.entropy(targs2)
                if gain > best_gain and len(feats1) > 0 and len(feats2) > 0:
                    best_gain = gain
                    best_criteria = (col, value)
                    best_sets = ((feats1, targs1), (feats2, targs2))

        if best_gain > 0:
            left_branch = self.build_tree(best_sets[0][0], best_sets[0][1], depth - 1)
            right_branch = self.build_tree(best_sets[1][0], best_sets[1][1], depth - 1)
            return DecisionNode(col=best_criteria[0], value=best_criteria[1],
                                tb=left_branch, fb=right_branch)
        else:
            return DecisionNode(result=max(self.unique_counts(targets)))
</code></pre>
  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#pylearning.trees.DecisionTreeClassifier">DecisionTreeClassifier</a></li>
          <li><a href="#pylearning.trees.DecisionTree">DecisionTree</a></li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, max_depth=-1, min_leaf_examples=6, max_split_features=&#39;auto&#39;)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.__init__', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.__init__" class="source">
    <pre><code>def __init__(self, max_depth=-1, min_leaf_examples=6, max_split_features="auto"):
    self.root_node = None
    self.max_depth = max_depth
    self.min_leaf_examples = min_leaf_examples
    if max_split_features in ["auto","sqrt","log2"] or \
        isinstance(max_split_features, int) or max_split_features is None:
        self.max_split_features = max_split_features
        self.considered_features = None
    else:
        raise ValueError("Argument max_split_features must be 'auto', \
                        'sqrt', 'log2', an int or None")
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.build_tree">
    <p>def <span class="ident">build_tree</span>(</p><p>self, features, targets, depth)</p>
    </div>
    

    
  
    <div class="desc"><p>Recursively create the decision tree by splitting the dataset until there
is no real reduce in variance, or there is less examples in a node than
the minimum number of examples, or until the max depth is reached.</p>
<p>:param features:    Array-like object of features of shape (nb_samples, nb_features)
:param targets:     Array-like object of target values of shape (nb_samples)
:param depth:       The current depth in the tree
:return:            The root node of the constructed tree</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.build_tree', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.build_tree" class="source">
    <pre><code>def build_tree(self, features, targets, depth):
    """
    Recursively create the decision tree by splitting the dataset until there
    is no real reduce in variance, or there is less examples in a node than
    the minimum number of examples, or until the max depth is reached.
    :param features:    Array-like object of features of shape (nb_samples, nb_features)
    :param targets:     Array-like object of target values of shape (nb_samples)
    :param depth:       The current depth in the tree
    :return:            The root node of the constructed tree
    """
    if len(features) == 0:
        return DecisionNode()
    if depth == 0:
        return DecisionNode(result=max(self.unique_counts(targets)))
    current_score = self.entropy(targets)
    best_gain = 0.0
    best_criteria = None
    best_sets = None
    considered_features = self.choose_random_features(features[0])
    for col in considered_features:
        column_values = set([row[col] for row in features])
        for value in column_values:
            feats1, targs1, feats2, targs2 = \
                self.divide_set(features, targets, col, value)
            p = float(len(feats1)) / len(features)
            gain = current_score - p * self.entropy(targs1) - \
                    (1 - p) * self.entropy(targs2)
            if gain > best_gain and len(feats1) > 0 and len(feats2) > 0:
                best_gain = gain
                best_criteria = (col, value)
                best_sets = ((feats1, targs1), (feats2, targs2))
    if best_gain > 0:
        left_branch = self.build_tree(best_sets[0][0], best_sets[0][1], depth - 1)
        right_branch = self.build_tree(best_sets[1][0], best_sets[1][1], depth - 1)
        return DecisionNode(col=best_criteria[0], value=best_criteria[1],
                            tb=left_branch, fb=right_branch)
    else:
        return DecisionNode(result=max(self.unique_counts(targets)))
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.choose_random_features">
    <p>def <span class="ident">choose_random_features</span>(</p><p>self, row)</p>
    </div>
    

    
  
    <div class="desc"><p>Randomly selects indexes in the given list. The number of indexes
chosen is the square root of the number of elements in the initial
list.
:param row: One-dimensional array
:return:    Array containing the chosen indexes</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.choose_random_features', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.choose_random_features" class="source">
    <pre><code>def choose_random_features(self, row):
    """
    Randomly selects indexes in the given list. The number of indexes
    chosen is the square root of the number of elements in the initial
    list.
    :param row: One-dimensional array
    :return:    Array containing the chosen indexes
    """
    return random.sample(range(len(row)), self.considered_features)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.divide_set">
    <p>def <span class="ident">divide_set</span>(</p><p>self, features, targets, column, feature_value)</p>
    </div>
    

    
  
    <div class="desc"><p>Divide the given dataset depending on the value at the given column index.
:param features:    Features of the dataset
:param targets:     Targets of the dataset
:param column:      The index of the column used to split data
:param value:       The value used for the split</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.divide_set', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.divide_set" class="source">
    <pre><code>def divide_set(self, features, targets, column, feature_value):
    """
    Divide the given dataset depending on the value at the given column index.
    :param features:    Features of the dataset
    :param targets:     Targets of the dataset
    :param column:      The index of the column used to split data
    :param value:       The value used for the split
    """
    split_function = None
    if isinstance(feature_value, int) or isinstance(feature_value, float):
        split_function = lambda row: row[column] >= feature_value
    else:
        split_function = lambda row: row[column] == feature_value
    set1 = [row for row in zip(features, targets) if split_function(row[0])]
    set2 = [row for row in zip(features, targets) if not split_function(row[0])]
    feat1, targs1 = [x[0] for x in set1], [x[1] for x in set1]
    feat2, targs2 = [x[0] for x in set2], [x[1] for x in set2]
    return feat1, targs1, feat2, targs2
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.entropy">
    <p>def <span class="ident">entropy</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the entropy in the given rows.
:param targets:     1D array-like targets
:return:            Float value of entropy</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.entropy', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.entropy" class="source">
    <pre><code>def entropy(self, targets):
    """
    Returns the entropy in the given rows.
    :param targets:     1D array-like targets
    :return:            Float value of entropy
    """
    results = self.unique_counts(targets)
    ent = 0.0
    for val in results.values():
        p = float(val) / len(targets)
        ent -= p * log2(p)
    return ent
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, features, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Trains the algorithm using the given dataset.
:param features:    Features used to train the tree.
                    Array-like object of dimensions (nb_samples, nb_features)
:param targets:     Target values corresponding to the features.
                    Array-like object of dimensions (n_samples)</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.fit', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.fit" class="source">
    <pre><code>def fit(self, features, targets):
    """
    Trains the algorithm using the given dataset.
    :param features:    Features used to train the tree.
                        Array-like object of dimensions (nb_samples, nb_features)
    :param targets:     Target values corresponding to the features.
                        Array-like object of dimensions (n_samples)
    """
    if len(features) < 1:
        raise ValueError("Not enough samples in the given dataset")
    self.set_number_features_evaluated_split(features[0])
    self.root_node = self.build_tree(features, targets, self.max_depth)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.get_features_subset">
    <p>def <span class="ident">get_features_subset</span>(</p><p>self, row)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the randomly selected values in the given features.
:param row: One-dimensional array of features</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.get_features_subset', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.get_features_subset" class="source">
    <pre><code>def get_features_subset(self, row):
    """
    Returns the randomly selected values in the given features.
    :param row: One-dimensional array of features
    """
    return [row[i] for i in self.features_indexes]
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.mean_output">
    <p>def <span class="ident">mean_output</span>(</p><p>self, targets)</p>
    </div>
    
    <p class="inheritance">
     <strong>Inheritance:</strong>
       <code><a href="#pylearning.trees.DecisionTree">DecisionTree</a></code>.<code><a href="#pylearning.trees.DecisionTree.mean_output">mean_output</a></code>
    </p>

    
  
    <div class="desc inherited"><p>Calculate the mean value of the given list.
:param targets: One-dimensional array of floats or ints
:return:        Float value</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.mean_output', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.mean_output" class="source">
    <pre><code>def mean_output(self, targets):
    """
    Calculate the mean value of the given list.
    :param targets: One-dimensional array of floats or ints
    :return:        Float value
    """
    return sum(targets) / len(targets)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.predict">
    <p>def <span class="ident">predict</span>(</p><p>self, features)</p>
    </div>
    

    
  
    <div class="desc"><p>Predict a value for the given features.
:param features:    Array of features of dimension (nb_features)
:return:            Float value or predicted class</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.predict', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.predict" class="source">
    <pre><code>def predict(self, features):
    """
    Predict a value for the given features.
    :param features:    Array of features of dimension (nb_features)
    :return:            Float value or predicted class
    """
    return self.propagate(features, self.root_node)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.propagate">
    <p>def <span class="ident">propagate</span>(</p><p>self, observation, tree)</p>
    </div>
    

    
  
    <div class="desc"><p>Makes a prediction using the given features.
:param  observation:    The features to use to predict
:param  tree:           The current node
:return:                Predicted value (float)</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.propagate', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.propagate" class="source">
    <pre><code>def propagate(self, observation, tree):
    """
    Makes a prediction using the given features.
    :param  observation:    The features to use to predict
    :param  tree:           The current node
    :return:                Predicted value (float)
    """
    if tree.result is not None:
        return tree.result
    else:
        v = observation[tree.col]
        branch = None
        if isinstance(v, int) or isinstance(v, float):
            if v >= tree.value:
                branch = tree.tb
            else:
                branch = tree.fb
        else:
            if v == tree.value:
                branch = tree.tb
            else:
                branch = tree.fb
        return self.propagate(observation, branch)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.set_number_features_evaluated_split">
    <p>def <span class="ident">set_number_features_evaluated_split</span>(</p><p>self, row)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the number of considered features at each split depending on the
max_split_features parameter.
:param row: A single row of the features of shape (nb_features)</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.set_number_features_evaluated_split', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.set_number_features_evaluated_split" class="source">
    <pre><code>def set_number_features_evaluated_split(self, row):
    """
    Sets the number of considered features at each split depending on the
    max_split_features parameter.
    :param row: A single row of the features of shape (nb_features)
    """
    if isinstance(self.max_split_features, int):
        self.considered_features = self.max_split_features if \
            self.max_split_features <= len(row) else len(row)
    elif isinstance(self.max_split_features, str):
        if self.max_split_features in ['auto','sqrt']:
            self.considered_features = int(sqrt(len(row)))
        elif self.max_split_features == 'log2':
            self.considered_features = int(log2(len(row)))
    else:
        self.considered_features = len(row)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.unique_counts">
    <p>def <span class="ident">unique_counts</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the occurence of each result in the given dataset.
:param targets:     1D array-like targets
:return:            Dictionary of target =&gt; number of occurences</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.unique_counts', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.unique_counts" class="source">
    <pre><code>def unique_counts(self, targets):
    """
    Returns the occurence of each result in the given dataset.
    :param targets:     1D array-like targets
    :return:            Dictionary of target => number of occurences
    """
    counts = {k: targets.count(k) for k in set(targets)}
    return counts
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeClassifier.variance">
    <p>def <span class="ident">variance</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Calculate the variance in the given list.
:param targets: One-dimensional array of float or ints
:return:        Float value</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeClassifier.variance', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeClassifier.variance" class="source">
    <pre><code>def variance(self, targets):
    """
    Calculate the variance in the given list.
    :param targets: One-dimensional array of float or ints
    :return:        Float value
    """
    if len(targets) == 0:
        return None
    mean = self.mean_output(targets)
    variance = sum([(x - mean)**2 for x in targets])
    return variance
</code></pre>
  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="pylearning.trees.DecisionTreeRegressor" class="name">class <span class="ident">DecisionTreeRegressor</span></p>
      
  
    <div class="desc"><p>:param  max_depth:          Maximum number of splits during training
:param min_leaf_examples:   Minimum number of examples in a leaf node.
:param max_split_features:  Maximum number of features considered at each
                            split (default='auto') :
                               - If int, the given number of will be used
                               - If 'auto' or 'sqrt', number of features
                                 considered = sqrt(nb_features)
                               - If 'log2', considered = log2(nb_features)
                               - If None, all features will be considered</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor" class="source">
    <pre><code>class DecisionTreeRegressor(DecisionTree):
    """
    :param  max_depth:          Maximum number of splits during training
    :param min_leaf_examples:   Minimum number of examples in a leaf node.
    :param max_split_features:  Maximum number of features considered at each
                                split (default='auto') :
                                   - If int, the given number of will be used
                                   - If 'auto' or 'sqrt', number of features
                                     considered = sqrt(nb_features)
                                   - If 'log2', considered = log2(nb_features)
                                   - If None, all features will be considered
    """


    def build_tree(self, features, targets, depth):
        """
        Recursively create the decision tree by splitting the dataset until there
        is no real reduce in variance, or there is less examples in a node than
        the minimum number of examples, or until the max depth is reached.
        :param features:    Array-like object of features of shape (nb_samples, nb_features)
        :param targets:     Array-like object of target values of shape (nb_samples)
        :param depth:       The current depth in the tree
        :return:            The root node of the constructed tree
        """
        if len(features) == 0:
            return DecisionNode()
        if depth == 0:
            return DecisionNode(result=self.mean_output(targets))

        lowest_variance = None
        best_criteria = None
        best_sets = None

        considered_features = self.choose_random_features(features[0])
        for column in considered_features:
            column_values = [feature[column] for feature in features]
            for feature_value in column_values:
                feats1, targs1, feats2, targs2 = \
                    self.divide_set(features, targets, column, feature_value)
                var1 = self.variance(targs1)
                var2 = self.variance(targs2)
                if var1 is None or var2 is None:
                    continue
                variance = var1 + var2
                if lowest_variance is None or variance < lowest_variance:
                    lowest_variance = variance
                    best_criteria   = (column, feature_value)
                    best_sets       = ((feats1, targs1),(feats2, targs2))

        # Check variance value also
        if lowest_variance is not None and \
            len(best_sets[0][0]) >= self.min_leaf_examples and \
            len(best_sets[1][0]) >= self.min_leaf_examples:
            left_branch = self.build_tree(best_sets[0][0], best_sets[0][1], depth - 1)
            right_branch = self.build_tree(best_sets[1][0], best_sets[1][1], depth - 1)
            return DecisionNode(col=best_criteria[0], value=best_criteria[1],
                                tb=left_branch, fb=right_branch)
        else:
            return DecisionNode(result=self.mean_output(targets))
</code></pre>
  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#pylearning.trees.DecisionTreeRegressor">DecisionTreeRegressor</a></li>
          <li><a href="#pylearning.trees.DecisionTree">DecisionTree</a></li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, max_depth=-1, min_leaf_examples=6, max_split_features=&#39;auto&#39;)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.__init__', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.__init__" class="source">
    <pre><code>def __init__(self, max_depth=-1, min_leaf_examples=6, max_split_features="auto"):
    self.root_node = None
    self.max_depth = max_depth
    self.min_leaf_examples = min_leaf_examples
    if max_split_features in ["auto","sqrt","log2"] or \
        isinstance(max_split_features, int) or max_split_features is None:
        self.max_split_features = max_split_features
        self.considered_features = None
    else:
        raise ValueError("Argument max_split_features must be 'auto', \
                        'sqrt', 'log2', an int or None")
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.build_tree">
    <p>def <span class="ident">build_tree</span>(</p><p>self, features, targets, depth)</p>
    </div>
    

    
  
    <div class="desc"><p>Recursively create the decision tree by splitting the dataset until there
is no real reduce in variance, or there is less examples in a node than
the minimum number of examples, or until the max depth is reached.
:param features:    Array-like object of features of shape (nb_samples, nb_features)
:param targets:     Array-like object of target values of shape (nb_samples)
:param depth:       The current depth in the tree
:return:            The root node of the constructed tree</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.build_tree', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.build_tree" class="source">
    <pre><code>def build_tree(self, features, targets, depth):
    """
    Recursively create the decision tree by splitting the dataset until there
    is no real reduce in variance, or there is less examples in a node than
    the minimum number of examples, or until the max depth is reached.
    :param features:    Array-like object of features of shape (nb_samples, nb_features)
    :param targets:     Array-like object of target values of shape (nb_samples)
    :param depth:       The current depth in the tree
    :return:            The root node of the constructed tree
    """
    if len(features) == 0:
        return DecisionNode()
    if depth == 0:
        return DecisionNode(result=self.mean_output(targets))
    lowest_variance = None
    best_criteria = None
    best_sets = None
    considered_features = self.choose_random_features(features[0])
    for column in considered_features:
        column_values = [feature[column] for feature in features]
        for feature_value in column_values:
            feats1, targs1, feats2, targs2 = \
                self.divide_set(features, targets, column, feature_value)
            var1 = self.variance(targs1)
            var2 = self.variance(targs2)
            if var1 is None or var2 is None:
                continue
            variance = var1 + var2
            if lowest_variance is None or variance < lowest_variance:
                lowest_variance = variance
                best_criteria   = (column, feature_value)
                best_sets       = ((feats1, targs1),(feats2, targs2))
    # Check variance value also
    if lowest_variance is not None and \
        len(best_sets[0][0]) >= self.min_leaf_examples and \
        len(best_sets[1][0]) >= self.min_leaf_examples:
        left_branch = self.build_tree(best_sets[0][0], best_sets[0][1], depth - 1)
        right_branch = self.build_tree(best_sets[1][0], best_sets[1][1], depth - 1)
        return DecisionNode(col=best_criteria[0], value=best_criteria[1],
                            tb=left_branch, fb=right_branch)
    else:
        return DecisionNode(result=self.mean_output(targets))
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.choose_random_features">
    <p>def <span class="ident">choose_random_features</span>(</p><p>self, row)</p>
    </div>
    

    
  
    <div class="desc"><p>Randomly selects indexes in the given list. The number of indexes
chosen is the square root of the number of elements in the initial
list.
:param row: One-dimensional array
:return:    Array containing the chosen indexes</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.choose_random_features', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.choose_random_features" class="source">
    <pre><code>def choose_random_features(self, row):
    """
    Randomly selects indexes in the given list. The number of indexes
    chosen is the square root of the number of elements in the initial
    list.
    :param row: One-dimensional array
    :return:    Array containing the chosen indexes
    """
    return random.sample(range(len(row)), self.considered_features)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.divide_set">
    <p>def <span class="ident">divide_set</span>(</p><p>self, features, targets, column, feature_value)</p>
    </div>
    

    
  
    <div class="desc"><p>Divide the given dataset depending on the value at the given column index.
:param features:    Features of the dataset
:param targets:     Targets of the dataset
:param column:      The index of the column used to split data
:param value:       The value used for the split</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.divide_set', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.divide_set" class="source">
    <pre><code>def divide_set(self, features, targets, column, feature_value):
    """
    Divide the given dataset depending on the value at the given column index.
    :param features:    Features of the dataset
    :param targets:     Targets of the dataset
    :param column:      The index of the column used to split data
    :param value:       The value used for the split
    """
    split_function = None
    if isinstance(feature_value, int) or isinstance(feature_value, float):
        split_function = lambda row: row[column] >= feature_value
    else:
        split_function = lambda row: row[column] == feature_value
    set1 = [row for row in zip(features, targets) if split_function(row[0])]
    set2 = [row for row in zip(features, targets) if not split_function(row[0])]
    feat1, targs1 = [x[0] for x in set1], [x[1] for x in set1]
    feat2, targs2 = [x[0] for x in set2], [x[1] for x in set2]
    return feat1, targs1, feat2, targs2
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.entropy">
    <p>def <span class="ident">entropy</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the entropy in the given rows.
:param targets:     1D array-like targets
:return:            Float value of entropy</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.entropy', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.entropy" class="source">
    <pre><code>def entropy(self, targets):
    """
    Returns the entropy in the given rows.
    :param targets:     1D array-like targets
    :return:            Float value of entropy
    """
    results = self.unique_counts(targets)
    ent = 0.0
    for val in results.values():
        p = float(val) / len(targets)
        ent -= p * log2(p)
    return ent
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, features, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Trains the algorithm using the given dataset.
:param features:    Features used to train the tree.
                    Array-like object of dimensions (nb_samples, nb_features)
:param targets:     Target values corresponding to the features.
                    Array-like object of dimensions (n_samples)</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.fit', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.fit" class="source">
    <pre><code>def fit(self, features, targets):
    """
    Trains the algorithm using the given dataset.
    :param features:    Features used to train the tree.
                        Array-like object of dimensions (nb_samples, nb_features)
    :param targets:     Target values corresponding to the features.
                        Array-like object of dimensions (n_samples)
    """
    if len(features) < 1:
        raise ValueError("Not enough samples in the given dataset")
    self.set_number_features_evaluated_split(features[0])
    self.root_node = self.build_tree(features, targets, self.max_depth)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.get_features_subset">
    <p>def <span class="ident">get_features_subset</span>(</p><p>self, row)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the randomly selected values in the given features.
:param row: One-dimensional array of features</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.get_features_subset', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.get_features_subset" class="source">
    <pre><code>def get_features_subset(self, row):
    """
    Returns the randomly selected values in the given features.
    :param row: One-dimensional array of features
    """
    return [row[i] for i in self.features_indexes]
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.mean_output">
    <p>def <span class="ident">mean_output</span>(</p><p>self, targets)</p>
    </div>
    
    <p class="inheritance">
     <strong>Inheritance:</strong>
       <code><a href="#pylearning.trees.DecisionTree">DecisionTree</a></code>.<code><a href="#pylearning.trees.DecisionTree.mean_output">mean_output</a></code>
    </p>

    
  
    <div class="desc inherited"><p>Calculate the mean value of the given list.
:param targets: One-dimensional array of floats or ints
:return:        Float value</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.mean_output', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.mean_output" class="source">
    <pre><code>def mean_output(self, targets):
    """
    Calculate the mean value of the given list.
    :param targets: One-dimensional array of floats or ints
    :return:        Float value
    """
    return sum(targets) / len(targets)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.predict">
    <p>def <span class="ident">predict</span>(</p><p>self, features)</p>
    </div>
    

    
  
    <div class="desc"><p>Predict a value for the given features.
:param features:    Array of features of dimension (nb_features)
:return:            Float value or predicted class</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.predict', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.predict" class="source">
    <pre><code>def predict(self, features):
    """
    Predict a value for the given features.
    :param features:    Array of features of dimension (nb_features)
    :return:            Float value or predicted class
    """
    return self.propagate(features, self.root_node)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.propagate">
    <p>def <span class="ident">propagate</span>(</p><p>self, observation, tree)</p>
    </div>
    

    
  
    <div class="desc"><p>Makes a prediction using the given features.
:param  observation:    The features to use to predict
:param  tree:           The current node
:return:                Predicted value (float)</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.propagate', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.propagate" class="source">
    <pre><code>def propagate(self, observation, tree):
    """
    Makes a prediction using the given features.
    :param  observation:    The features to use to predict
    :param  tree:           The current node
    :return:                Predicted value (float)
    """
    if tree.result is not None:
        return tree.result
    else:
        v = observation[tree.col]
        branch = None
        if isinstance(v, int) or isinstance(v, float):
            if v >= tree.value:
                branch = tree.tb
            else:
                branch = tree.fb
        else:
            if v == tree.value:
                branch = tree.tb
            else:
                branch = tree.fb
        return self.propagate(observation, branch)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.set_number_features_evaluated_split">
    <p>def <span class="ident">set_number_features_evaluated_split</span>(</p><p>self, row)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the number of considered features at each split depending on the
max_split_features parameter.
:param row: A single row of the features of shape (nb_features)</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.set_number_features_evaluated_split', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.set_number_features_evaluated_split" class="source">
    <pre><code>def set_number_features_evaluated_split(self, row):
    """
    Sets the number of considered features at each split depending on the
    max_split_features parameter.
    :param row: A single row of the features of shape (nb_features)
    """
    if isinstance(self.max_split_features, int):
        self.considered_features = self.max_split_features if \
            self.max_split_features <= len(row) else len(row)
    elif isinstance(self.max_split_features, str):
        if self.max_split_features in ['auto','sqrt']:
            self.considered_features = int(sqrt(len(row)))
        elif self.max_split_features == 'log2':
            self.considered_features = int(log2(len(row)))
    else:
        self.considered_features = len(row)
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.unique_counts">
    <p>def <span class="ident">unique_counts</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the occurence of each result in the given dataset.
:param targets:     1D array-like targets
:return:            Dictionary of target =&gt; number of occurences</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.unique_counts', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.unique_counts" class="source">
    <pre><code>def unique_counts(self, targets):
    """
    Returns the occurence of each result in the given dataset.
    :param targets:     1D array-like targets
    :return:            Dictionary of target => number of occurences
    """
    counts = {k: targets.count(k) for k in set(targets)}
    return counts
</code></pre>
  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="pylearning.trees.DecisionTreeRegressor.variance">
    <p>def <span class="ident">variance</span>(</p><p>self, targets)</p>
    </div>
    

    
  
    <div class="desc"><p>Calculate the variance in the given list.
:param targets: One-dimensional array of float or ints
:return:        Float value</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-pylearning.trees.DecisionTreeRegressor.variance', this);">Show source &equiv;</a></p>
  <div id="source-pylearning.trees.DecisionTreeRegressor.variance" class="source">
    <pre><code>def variance(self, targets):
    """
    Calculate the variance in the given list.
    :param targets: One-dimensional array of float or ints
    :return:        Float value
    """
    if len(targets) == 0:
        return None
    mean = self.mean_output(targets)
    variance = sum([(x - mean)**2 for x in targets])
    return variance
</code></pre>
  </div>
</div>

  </div>
  
      </div>
      </div>

  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>pdoc is in the public domain with the
      <a href="http://unlicense.org">UNLICENSE</a></p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
